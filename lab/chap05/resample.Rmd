---
title: "ISLR实验：交叉验证法和自助法"
output: html_notebook
---

```{r}
library(ISLR)
```

## 验证集方法

validation set approach

随机吧观测集分成两部分：一个训练集 (training set) 和一个验证集 (validation set)，或叫做保留集 (hold-out set)。
在训练集上拟合模型，用验证集计算错误率 (通常使用均方误差)

```{r paged.print=FALSE}
head(Auto)
```

```{r}
dim(Auto)
```

随机抽取训练集，使用 `sample()` 函数生成样本抽样序号子集。
为保证结果一致，使用 `set.seed()` 函数设置固定的随机数种子。

```{r}
set.seed(1)
train <- sample(nrow(Auto), nrow(Auto)/2)
```

拟合线性模型

```{r}
lm_fit <- lm(
  mpg ~ horsepower,
  data=Auto,
  subset=train
)
```

计算验证集的均方误差

```{r}
mean((Auto$mpg - predict(lm_fit, Auto))[-train]^2)
```

计算二次和三次多项式回归的均方误差

```{r}
lm_fit_p2 <- lm(
  mpg ~ poly(horsepower, 2),
  data=Auto,
  subset=train
)
mean((Auto$mpg - predict(lm_fit_p2, Auto))[-train]^2)
```

```{r}
lm_fit_p3 <- lm(
  mpg ~ poly(horsepower,3),
  data=Auto,
  subset=train
)
mean((Auto$mpg - predict(lm_fit_p3, Auto))[-train]^2)
```

使用不同的训练集，在验证集上会得到不同的结果

```{r}
set.seed(2)
train <- sample(nrow(Auto), nrow(Auto)/2)
```

```{r}
lm_fit <- lm(
  mpg ~ horsepower,
  data=Auto,
  subset=train
)
mean((Auto$mpg - predict(lm_fit, Auto))[-train]^2)
```

```{r}
lm_fit_p2 <- lm(
  mpg ~ poly(horsepower,2),
  data=Auto,
  subset=train
)
mean((Auto$mpg - predict(lm_fit_p2, Auto))[-train]^2)
```

```{r}
lm_fit_p3 <- lm(
  mpg ~ poly(horsepower,3),
  data=Auto,
  subset=train
)
mean((Auto$mpg - predict(lm_fit_p3, Auto))[-train]^2)
```

## 留一交叉验证法

leave-one-out cross-validation, LOOCV

将一个单独的观测作为验证集，剩下的观测作为训练集。
多次重复该步骤，计算测试均方误差的 LOOCV 估计：

$$
CV_{(n)} = \frac {1} {n} \sum_{i=1}^n MSE_i
$$

不设置 `family` 参数的 `glm()` 函数与 `lm()` 函数一样执行线性回归

```{r}
glm_fit <- glm(
  mpg ~ horsepower,
  data=Auto
)
coefficients(glm_fit)
```

```{r}
lm_fit <- lm(
  mpg ~ horsepower,
  data=Auto
)
coefficients(lm_fit)
```

`glm()` 与 `boot` 包的 `cv.glm()` 函数可以计算 LOOCV 估计

```{r}
library(boot)
```

`cv.glm()` 函数返回结果中 `delta` 向量的两个数字为交叉验证的结果。
本例中两个数字相等

```{r}
glm_fit <- glm(
  mpg ~ horsepower,
  data=Auto
)
cv.err <- cv.glm(
  Auto,
  glm_fit
)
cv.err$delta
```

用递增的多项式次数重复上述步骤

```{r}
cv.error <- rep(0, 5)
for (i in 1:5) {
  glm_fit <- glm(
    mpg ~ poly(horsepower, i),
    data=Auto
  )
  cv.error[i] <- cv.glm(
    Auto,
    glm_fit
  )$delta[1]
}
cv.error
```

可以看到，超过二次的多项式拟合效果没有显著的提升

## k 折交叉验证法

k-fold CV

将观测集随机地分为 k 个大小基本一致的组，或者说折 (fold)。
将某一个折作为验证集，其余折作为训练集。
重复该步骤 k 次，得到 k 折 CV 估计：

$$
CV_{(k)} = \frac {1} {k} \sum_{i=1}^k MSE_{i}
$$

`cv.glm()` 函数也可以实现 k 折交叉验证法。
通常 k 取 10。

```{r}
set.seed(17)
cv_error_10 <- NULL
for (i in 1:10) {
  glm_fit <- glm(
    mpg ~ poly(horsepower, i),
    data=Auto
  )
  cv_error_10 <- rbind(
    cv_error_10,
    cv.glm(
      Auto,
      glm_fit,
      K=10
    )$delta
  )
}
cv_error_10
```

第一列是标准 k 折 CV 估计，第二列是偏差校正后的结果。
本例中两者区别不大。

## 自助法

bootstrap

### 估计一个感兴趣的统计量的精度

金融资产数据集，X 和 Y 是两种金融资产的收益

```{r paged.print=FALSE}
head(Portfolio)
```

```{r}
dim(Portfolio)
```

alpha 估计，使总风险或者说是总方差最小

```{r}
alpha_fn <- function(data, index) {
  X <- data$X[index]
  Y <- data$Y[index]
  return(
    (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2*cov(X, Y))
  )
}
```

用全部数据估计

```{r}
alpha_fn(Portfolio, 1:100)
```

`sample()` 有放回地取出 100 个观测进行估计

```{r}
set.seed(1)
alpha_fn(Portfolio, sample(100, 100, replace=TRUE))
```

自助法，重复 1000 次上述步骤

```{r}
boot_result <- boot(
  Portfolio,
  alpha_fn,
  R=1000
)
boot_result
```

估计 alpha 为 0.5758，标准误为 0.0937

使用 `boot.ci()` 函数求 alpha 的 95% 置信区间

```{r}
boot.ci(boot_result, type="norm")
```

### 估计线性回归模型的精度

衡量线性回归模型 beta_0 和 beta_1 估计的波动性

```{r}
boot_fn <- function(data, index) {
  return (coef(lm(
    mpg ~ horsepower,
    data=Auto,
    subset=index
  )))
}
```

全部数据

```{r}
boot_fn(Auto, 1:nrow(Auto))
```

随机有放回的两个例子

```{r}
set.seed(1)
boot_fn(
  Auto, 
  sample(
    nrow(Auto), 
    nrow(Auto), 
    replace=TRUE
  )
)
boot_fn(
  Auto, 
  sample(
    nrow(Auto), 
    nrow(Auto), 
    replace=TRUE
  )
)
```

计算 1000 个截距和斜率

```{r}
boot(
  Auto,
  boot_fn,
  R=1000
)
```

用标准公式计算线性模型中截距和回归系数的标准误差

```{r}
summary(
  lm(
    mpg ~ horsepower,
    data=Auto
  )
)$coef
```

计算结果与自助法不同。
实际上标准公式需要数据满足某些假设，而自助法则没有这些限制。
数据有非线性关系，所以自助法得到的结果更接近真实值。

拟合二次模型的结果

```{r}
boot_fn <- function(data, index) {
  return (coefficients(
    lm(
      mpg ~ horsepower + I(horsepower^2),
      data=data,
      subset=index
    )
  ))
}
set.seed(1)
boot(Auto, boot_fn, R=1000)
```

标准公式计算

```{r}
summary(
  lm(
    mpg ~ horsepower + I(horsepower^2),
    data=Auto
  )
)$coef
```

二次模型拟合效果比线性模型更好，所以自助法估计与标准估计更接近
